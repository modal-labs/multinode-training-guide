# SLIME + Harbor + Modal code-golf example (Qwen 8B)

This example trains **Qwen3-8B** with SLIME on Modal for Python code golf.

It does two things in reward:

1. **Correctness** via Harbor task verification.
2. **Code-size pressure** via a length bonus on top of correctness.

## What this example adds

- MBPP (`Muennighoff/mbpp`) conversion into:
  - **Harbor task project format** (`tasks/<task_name>/...`)
  - **SLIME parquet prompt format** (`slime/train.parquet`, `slime/test.parquet`)
- Regex function-name extraction from MBPP `code` field and injection into prompt.
- Custom SLIME reward model (`--custom-rm-path`) that runs Harbor tests in **Modal sandboxes**.
- Harbor eval path with:
  - a custom single-shot code agent (`harbor_litellm_agent.py`)
  - a checkpoint server (SGLang) that serves the latest checkpoint from Modal volume
  - high-concurrency Harbor runs (`-n` with `--env modal`)

## Files

- `modal_train.py` – Modal entrypoints for dataset prep, training, serving, and eval.
- `dataset_to_harbor.py` – MBPP -> Harbor + SLIME conversion function.
- `custom_rm.py` – SLIME custom reward function (Harbor+Modal sandbox scoring + length bonus).
- `harbor_litellm_agent.py` – Harbor custom agent that calls an OpenAI-compatible endpoint.
- `configs/qwen_8b_multi.py` – Qwen3-8B multi-node config based on the Modal SLIME config style.

## 1) Prepare dataset in Modal volume

Run once:

`modal run slime/code_golf_harbor_modal/modal_train.py::prepare_dataset`

Optional small smoke conversion:

`modal run slime/code_golf_harbor_modal/modal_train.py::prepare_dataset --limit 32 --train-size 24`

Data lands in Modal volume `slime-code-golf-harbor-data` under:

- `/data/mbpp_harbor/tasks`
- `/data/mbpp_harbor/slime/train.parquet`
- `/data/mbpp_harbor/slime/test.parquet`

Each Harbor task uses `environment/Dockerfile` with:

`FROM python:3.11-slim`

## 2) Download base model

Run once:

`modal run slime/code_golf_harbor_modal/modal_train.py::download_model --config qwen-8b-multi`

## 3) Train with SLIME

`modal run slime/code_golf_harbor_modal/modal_train.py::train_multi_node --config qwen-8b-multi`

The config uses:

- `--custom-rm-path custom_rm.custom_rm`
- MBPP parquet generated by `prepare_dataset`
- Multi-node Qwen3-8B settings in the same style as the existing SLIME Modal configs

## 4) Serve latest checkpoint

Start a long-lived server endpoint from latest checkpoint:

`modal serve slime/code_golf_harbor_modal/modal_train.py::serve_latest_checkpoint`

Or one-shot eval (starts local server inside function, runs Harbor, exits):

`modal run slime/code_golf_harbor_modal/modal_train.py::eval_latest_checkpoint --n-concurrent 256 --n-tasks 500`

## 5) Run Harbor eval against server (Modal sandboxes)

Given a running server URL:

`modal run slime/code_golf_harbor_modal/modal_train.py::run_harbor_eval --server-base-url https://<your-modal-url> --n-concurrent 256 --n-tasks 500`

This uses Harbor with:

- `--env modal` (trial environments are Modal sandboxes)
- custom agent import path `harbor_litellm_agent.SingleShotCodeAgent`
- OpenAI-compatible API calls to the served checkpoint endpoint

## Reward behavior

The custom RM in `custom_rm.py` computes:

- `pass_rate` from Harbor task verifier output.
- `length_bonus` from `reference_bytes / candidate_bytes` (capped).
- Final reward: correctness scaled by a positive length bonus.

So passing tests remains primary; shorter passing programs get higher reward.
