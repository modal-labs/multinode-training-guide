"""
Model configurations and parallelism defaults.

Centralizes model-specific constants: expert counts, parallelism, model paths.
Scripts reference this file for correct values but define their own configs
inline (Modal scripts run as separate apps/containers).
"""

MODEL_CONFIGS = {
    "glm47": {
        "hf_model": "zai-org/GLM-4.7",
        "num_layers": 92,
        "num_experts": 160,  # n_routed_experts (plus 1 shared expert)
        "first_k_dense_replace": 3,
        "intermediate_size": 12288,
        "moe_intermediate_size": 1536,
        "hidden_size": 5120,
        "train_parallelism": {"tp": 2, "pp": 4, "ep": 4, "cp": 4, "gpus": 32},
        "infer_parallelism": {"tp": 2, "pp": 1, "ep": 4, "cp": 1, "gpus": 8},
        "max_seq_len": 131072,
        "gpu_type": "H200",
        "extra_pip": ["sentencepiece>=0.2.0"],
        # Volume naming conventions
        "checkpoint_prefix": "glm47_lora_",
        "msswift_checkpoint_prefix": "msswift_glm47_",
        "merged_prefix": "glm47-merged-",
        "wandb_project": "glm47-lora",
    },
    "qwen3-1.7b": {
        "hf_model": "Qwen/Qwen3-1.7B",
        "num_layers": 28,
        "num_experts": 0,
        "hidden_size": 2048,
        "max_seq_len": 131072,
        "gpu_type": "H100",
        "train_parallelism": {"tp": 1, "pp": 1, "ep": 1, "cp": 1, "gpus": 1},
        "infer_parallelism": {"tp": 1, "pp": 1, "ep": 1, "cp": 1, "gpus": 1},
        "extra_pip": [],
        "checkpoint_prefix": "qwen3_1.7b_",
        "merged_prefix": "qwen3-1.7b-merged-",
        "wandb_project": "qwen-sft",
    },
    "qwen3-4b": {
        "hf_model": "Qwen/Qwen3-4B",
        "num_layers": 36,
        "num_experts": 0,
        "hidden_size": 2560,
        "max_seq_len": 131072,
        "gpu_type": "H100",
        "train_parallelism": {"tp": 1, "pp": 1, "ep": 1, "cp": 1, "gpus": 1},
        "infer_parallelism": {"tp": 1, "pp": 1, "ep": 1, "cp": 1, "gpus": 1},
        "extra_pip": [],
        "checkpoint_prefix": "qwen3_4b_",
        "merged_prefix": "qwen3-4b-merged-",
        "wandb_project": "qwen-sft",
    },
    "qwen3-30b-a3b": {
        "hf_model": "Qwen/Qwen3-30B-A3B",
        "num_layers": 48,
        "num_experts": 128,
        "hidden_size": 2048,
        "moe_intermediate_size": 768,
        "max_seq_len": 131072,
        "gpu_type": "H100",
        "train_parallelism": {"tp": 2, "pp": 1, "ep": 4, "cp": 1, "gpus": 8},
        "infer_parallelism": {"tp": 2, "pp": 1, "ep": 1, "cp": 1, "gpus": 2},
        "extra_pip": [],
        "checkpoint_prefix": "qwen3_30b_",
        "merged_prefix": "qwen3-30b-merged-",
        "wandb_project": "qwen-lora",
    },
    "qwen3-235b-a22b": {
        "hf_model": "Qwen/Qwen3-235B-A22B",
        "num_layers": 94,
        "num_experts": 128,
        "hidden_size": 4096,
        "moe_intermediate_size": 2048,
        "max_seq_len": 131072,
        "gpu_type": "H200",
        "train_parallelism": {"tp": 2, "pp": 1, "ep": 8, "cp": 1, "gpus": 16},
        "infer_parallelism": {"tp": 8, "pp": 1, "ep": 1, "cp": 1, "gpus": 8},
        "extra_pip": [],
        "checkpoint_prefix": "qwen235b_",
        "msswift_checkpoint_prefix": "msswift_qwen235b_",
        "merged_prefix": "qwen235b-merged-",
        "wandb_project": "qwen-lora",
    },
    "qwen3-coder-next": {
        "hf_model": "Qwen/Qwen3-Coder-Next",
        "num_layers": 48,
        "num_experts": 512,
        "hidden_size": 2048,
        "moe_intermediate_size": 512,
        "max_seq_len": 131072,
        "gpu_type": "B200",
        "train_parallelism": {"tp": 1, "pp": 1, "ep": 4, "cp": 1, "gpus": 16},
        "infer_parallelism": {"tp": 4, "pp": 1, "ep": 1, "cp": 1, "gpus": 4},
        "extra_pip": [],
        "checkpoint_prefix": "coder_next_",
        "msswift_checkpoint_prefix": "msswift_coder_next_",
        "merged_prefix": "coder-next-merged-",
        "wandb_project": "qwen-coder-next-sft",
    },
}

# No bare "qwen" key â€” it's ambiguous between dense (1.7B/4B/30B) and 235B MoE.